{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Networks: Training with Backpropagation\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "1. Understand the theory behind backpropagation and gradient descent\n",
        "2. Implement backpropagation from scratch for a 2-layer neural network\n",
        "3. Train a neural network using gradient descent\n",
        "4. Build and train the same network using PyTorch\n",
        "5. Compare manual implementation with modern deep learning frameworks\n",
        "\n",
        "## Recap: Forward Propagation\n",
        "\n",
        "In the previous notebook (**16_neural_network_intro.ipynb**), we learned:\n",
        "- The structure of a 2-layer neural network\n",
        "- How forward propagation computes predictions\n",
        "- The role of weights, biases, and activation functions\n",
        "\n",
        "However, with randomly initialized weights, our predictions are essentially random! We need a way to **learn** the right weights from data.\n",
        "\n",
        "## The Training Problem\n",
        "\n",
        "**Goal**: Find weights and biases that minimize the difference between predictions and true labels.\n",
        "\n",
        "**Process**:\n",
        "1. **Measure error**: Use a loss function to quantify how wrong our predictions are\n",
        "2. **Compute gradients**: Use backpropagation to calculate how each parameter affects the loss\n",
        "3. **Update parameters**: Use gradient descent to adjust parameters in the direction that reduces loss\n",
        "4. **Repeat**: Iterate until the loss converges to a minimum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation\n",
        "\n",
        "We'll use the same breast cancer dataset and preprocessing as in the previous notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare the data\n",
        "data = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/changyaochen/MECE4520/master/\"\n",
        "    \"data/breast_cancer.csv\"\n",
        ")\n",
        "data[\"label\"] = data[\"diagnosis\"].apply(lambda x: 0 if x == \"B\" else 1)\n",
        "\n",
        "# Select features\n",
        "features = [\n",
        "    \"radius_mean\",\n",
        "    \"texture_mean\",\n",
        "    \"perimeter_mean\",\n",
        "    \"area_mean\",\n",
        "    \"smoothness_mean\",\n",
        "    \"compactness_mean\",\n",
        "    \"concavity_mean\",\n",
        "    \"concave_mean\",\n",
        "    \"symmetry_mean\",\n",
        "    \"fractal_mean\",\n",
        "]\n",
        "\n",
        "# Train-test split\n",
        "X_raw, X_raw_test, y, y_test = train_test_split(\n",
        "    data[features].values,\n",
        "    data[\"label\"].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X_raw)\n",
        "X_test = scaler.transform(X_raw_test)\n",
        "\n",
        "# Reshape labels\n",
        "y = y.reshape((-1, 1))\n",
        "y_test = y_test.reshape((-1, 1))\n",
        "\n",
        "print(f\"Training data: X={X.shape}, y={y.shape}\")\n",
        "print(f\"Test data: X_test={X_test.shape}, y_test={y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "Let's define the sigmoid activation function and the forward propagation function from the previous notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Computes the sigmoid activation function.\n",
        "\n",
        "    Args:\n",
        "        z: Input array of any shape.\n",
        "\n",
        "    Returns:\n",
        "        Array of same shape as z with sigmoid applied element-wise.\n",
        "    \"\"\"\n",
        "    return 1.0 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "def forward_propagation(\n",
        "    X: np.ndarray,\n",
        "    W_1: np.ndarray,\n",
        "    b_1: np.ndarray,\n",
        "    W_2: np.ndarray,\n",
        "    b_2: np.ndarray,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Performs forward propagation through a 2-layer neural network.\n",
        "\n",
        "    Args:\n",
        "        X: Input data of shape (n_samples, n_features).\n",
        "        W_1: Weight matrix for layer 1 of shape (n_hidden, n_features).\n",
        "        b_1: Bias vector for layer 1 of shape (n_hidden, 1).\n",
        "        W_2: Weight matrix for layer 2 of shape (1, n_hidden).\n",
        "        b_2: Bias scalar for layer 2 of shape (1, 1).\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "            - A_2: Final output predictions of shape (n_samples, 1).\n",
        "            - Z_2: Pre-activation values for layer 2 of shape (n_samples, 1).\n",
        "            - A_1: Activations from layer 1 of shape (n_samples, n_hidden).\n",
        "            - Z_1: Pre-activation values for layer 1 of shape (n_samples, n_hidden).\n",
        "    \"\"\"\n",
        "    # Layer 1: Hidden layer\n",
        "    Z_1 = X @ W_1.T\n",
        "    A_1 = sigmoid(Z_1 + b_1.T)\n",
        "\n",
        "    # Layer 2: Output layer\n",
        "    Z_2 = A_1 @ W_2.T\n",
        "    A_2 = sigmoid(Z_2 + b_2.T)\n",
        "\n",
        "    return A_2, Z_2, A_1, Z_1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backpropagation Theory\n",
        "\n",
        "### The Loss Function\n",
        "\n",
        "For binary classification, we use **binary cross-entropy loss** (also called log loss):\n",
        "\n",
        "\\[\n",
        "L = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) \\right]\n",
        "\\]\n",
        "\n",
        "This loss:\n",
        "- Is always positive\n",
        "- Equals 0 when predictions perfectly match labels\n",
        "- Increases as predictions diverge from labels\n",
        "- Works well with sigmoid activation\n",
        "\n",
        "### The Chain Rule\n",
        "\n",
        "Backpropagation uses the **chain rule** from calculus to compute how the loss changes with respect to each parameter. For a composed function $ f(g(x)) $, the chain rule states:\n",
        "\n",
        "\\[\n",
        "\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}\n",
        "\\]\n",
        "\n",
        "In our network, information flows: **Input → Layer 1 → Layer 2 → Output → Loss**\n",
        "\n",
        "To find how the loss changes with parameters in Layer 1, we need to propagate derivatives backward through Layer 2, Layer 1, and so on.\n",
        "\n",
        "### Gradient Descent\n",
        "\n",
        "Once we have gradients (derivatives of loss with respect to parameters), we update parameters using:\n",
        "\n",
        "\\[\n",
        "\\theta_{new} = \\theta_{old} - \\alpha \\frac{\\partial L}{\\partial \\theta}\n",
        "\\]\n",
        "\n",
        "where $ \\alpha $ is the **learning rate** (step size). The negative sign means we move in the direction that reduces the loss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backpropagation Implementation\n",
        "\n",
        "Now let's implement backpropagation to compute all the gradients. The key insight is to work **backward** from the output to the input.\n",
        "\n",
        "### Gradient Equations (for our 2-layer network with sigmoid activation):\n",
        "\n",
        "**Output layer (Layer 2):**\n",
        "- $ \\frac{\\partial L}{\\partial Z_2} = \\hat{Y} - Y $  (derivative of cross-entropy + sigmoid)\n",
        "- $ \\frac{\\partial L}{\\partial W_2} = \\frac{1}{n} \\frac{\\partial L}{\\partial Z_2}^T A_1 $\n",
        "- $ \\frac{\\partial L}{\\partial b_2} = \\frac{1}{n} \\text{mean}(\\frac{\\partial L}{\\partial Z_2}) $\n",
        "\n",
        "**Hidden layer (Layer 1):**\n",
        "- $ \\frac{\\partial L}{\\partial Z_1} = (\\frac{\\partial L}{\\partial Z_2} W_2) \\odot A_1 \\odot (1 - A_1) $  (chain rule + sigmoid derivative)\n",
        "- $ \\frac{\\partial L}{\\partial W_1} = \\frac{1}{n} \\frac{\\partial L}{\\partial Z_1}^T X $\n",
        "- $ \\frac{\\partial L}{\\partial b_1} = \\frac{1}{n} \\text{mean}(\\frac{\\partial L}{\\partial Z_1}) $\n",
        "\n",
        "where $ \\odot $ denotes element-wise multiplication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gradients_backprop(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    W_1: np.ndarray,\n",
        "    b_1: np.ndarray,\n",
        "    W_2: np.ndarray,\n",
        "    b_2: np.ndarray,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, float]:\n",
        "    \"\"\"Computes gradients using backpropagation for a 2-layer network.\n",
        "\n",
        "    Applies the chain rule to compute how the loss changes with respect to\n",
        "    each parameter. Uses binary cross-entropy loss for binary classification.\n",
        "\n",
        "    Args:\n",
        "        X: Input data of shape (n_samples, n_features).\n",
        "        y: True labels of shape (n_samples, 1).\n",
        "        W_1: Weight matrix for layer 1 of shape (n_hidden, n_features).\n",
        "        b_1: Bias vector for layer 1 of shape (n_hidden, 1).\n",
        "        W_2: Weight matrix for layer 2 of shape (1, n_hidden).\n",
        "        b_2: Bias scalar for layer 2 of shape (1, 1).\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "            - dW_2: Gradient for W_2 of shape (1, n_hidden).\n",
        "            - db_2: Gradient for b_2 of shape (1, 1).\n",
        "            - dW_1: Gradient for W_1 of shape (n_hidden, n_features).\n",
        "            - db_1: Gradient for b_1 of shape (n_hidden, 1).\n",
        "            - loss: Current loss value (scalar).\n",
        "    \"\"\"\n",
        "    # Forward propagation to get predictions and intermediate values\n",
        "    y_hat, Z_2, A_1, Z_1 = forward_propagation(X=X, W_1=W_1, b_1=b_1, W_2=W_2, b_2=b_2)\n",
        "    n = len(y_hat)\n",
        "\n",
        "    # Compute binary cross-entropy loss\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-15\n",
        "    y_hat_clipped = np.clip(y_hat, epsilon, 1 - epsilon)\n",
        "    loss = -np.mean(\n",
        "        y * np.log(y_hat_clipped) + (1 - y) * np.log(1 - y_hat_clipped)\n",
        "    )\n",
        "\n",
        "    # Backpropagation: Layer 2 (Output Layer)\n",
        "    # Derivative of loss with respect to Z_2 (combining cross-entropy + sigmoid)\n",
        "    dZ_2 = y_hat - y\n",
        "\n",
        "    # Gradients for W_2 and b_2\n",
        "    dW_2 = (dZ_2.T @ A_1) / n\n",
        "    db_2 = np.mean(dZ_2.T, axis=1, keepdims=True)\n",
        "\n",
        "    # Backpropagation: Layer 1 (Hidden Layer)\n",
        "    # Apply chain rule: derivative flows through W_2 and through sigmoid\n",
        "    # Sigmoid derivative: σ'(z) = σ(z) * (1 - σ(z)) = A_1 * (1 - A_1)\n",
        "    dZ_1 = (dZ_2 @ W_2) * A_1 * (1 - A_1)\n",
        "\n",
        "    # Gradients for W_1 and b_1\n",
        "    dW_1 = (dZ_1.T @ X) / n\n",
        "    db_1 = np.mean(dZ_1.T, axis=1, keepdims=True)\n",
        "\n",
        "    return dW_2, db_2, dW_1, db_1, loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Backpropagation\n",
        "\n",
        "Let's verify that our backpropagation implementation works by computing gradients with random initial parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize random parameters\n",
        "np.random.seed(42)\n",
        "W_1_init = np.random.normal(size=(3, X.shape[1]))\n",
        "b_1_init = np.random.normal(size=(3, 1))\n",
        "W_2_init = np.random.normal(size=(1, 3))\n",
        "b_2_init = np.random.normal(size=(1, 1))\n",
        "\n",
        "# Compute gradients\n",
        "dW_2, db_2, dW_1, db_1, initial_loss = compute_gradients_backprop(\n",
        "    X=X, y=y, W_1=W_1_init, b_1=b_1_init, W_2=W_2_init, b_2=b_2_init\n",
        ")\n",
        "\n",
        "print(\"Backpropagation Results:\")\n",
        "print(f\"Initial loss: {initial_loss:.4f}\")\n",
        "print(f\"\\nGradient shapes:\")\n",
        "print(f\"  dW_2: {dW_2.shape}, db_2: {db_2.shape}\")\n",
        "print(f\"  dW_1: {dW_1.shape}, db_1: {db_1.shape}\")\n",
        "print(f\"\\nSample gradient values:\")\n",
        "print(f\"  dW_2 mean: {np.mean(np.abs(dW_2)):.6f}\")\n",
        "print(f\"  dW_1 mean: {np.mean(np.abs(dW_1)):.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training with Gradient Descent\n",
        "\n",
        "Now we'll implement the complete training loop that:\n",
        "1. Computes gradients using backpropagation\n",
        "2. Updates parameters in the direction that reduces loss\n",
        "3. Repeats until convergence\n",
        "\n",
        "### Convergence Criteria\n",
        "\n",
        "We stop training when the change in loss becomes smaller than a threshold (`epsilon`), indicating we've reached a (local) minimum.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_neural_network(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    W_1_init: np.ndarray,\n",
        "    b_1_init: np.ndarray,\n",
        "    W_2_init: np.ndarray,\n",
        "    b_2_init: np.ndarray,\n",
        "    learning_rate: float = 0.01,\n",
        "    epsilon: float = 1e-6,\n",
        "    max_iterations: int = 10000,\n",
        "    verbose: bool = False,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, list, list]:\n",
        "    \"\"\"Trains a 2-layer neural network using gradient descent.\n",
        "\n",
        "    Iteratively computes gradients via backpropagation and updates parameters\n",
        "    to minimize the binary cross-entropy loss. Stops when loss converges or\n",
        "    maximum iterations is reached.\n",
        "\n",
        "    Args:\n",
        "        X: Training data of shape (n_samples, n_features).\n",
        "        y: Training labels of shape (n_samples, 1).\n",
        "        W_1_init: Initial weights for layer 1 of shape (n_hidden, n_features).\n",
        "        b_1_init: Initial biases for layer 1 of shape (n_hidden, 1).\n",
        "        W_2_init: Initial weights for layer 2 of shape (1, n_hidden).\n",
        "        b_2_init: Initial biases for layer 2 of shape (1, 1).\n",
        "        learning_rate: Step size for gradient descent updates.\n",
        "        epsilon: Convergence threshold for change in loss.\n",
        "        max_iterations: Maximum number of training iterations.\n",
        "        verbose: If True, prints progress every 100 iterations.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "            - W_1: Trained weights for layer 1.\n",
        "            - b_1: Trained biases for layer 1.\n",
        "            - W_2: Trained weights for layer 2.\n",
        "            - b_2: Trained biases for layer 2.\n",
        "            - losses: List of loss values at each iteration.\n",
        "            - roc_auc_scores: List of ROC-AUC scores at each iteration.\n",
        "    \"\"\"\n",
        "    # Initialize parameters\n",
        "    W_1 = W_1_init.copy()\n",
        "    b_1 = b_1_init.copy()\n",
        "    W_2 = W_2_init.copy()\n",
        "    b_2 = b_2_init.copy()\n",
        "\n",
        "    # Track metrics\n",
        "    losses = [float(\"inf\")]\n",
        "    roc_auc_scores = [0.5]\n",
        "\n",
        "    iteration = 0\n",
        "    diff_in_loss = float(\"inf\")\n",
        "\n",
        "    while abs(diff_in_loss) > epsilon and iteration < max_iterations:\n",
        "        iteration += 1\n",
        "\n",
        "        # Compute gradients using backpropagation\n",
        "        dW_2, db_2, dW_1, db_1, loss = compute_gradients_backprop(\n",
        "            X=X, y=y, W_1=W_1, b_1=b_1, W_2=W_2, b_2=b_2\n",
        "        )\n",
        "\n",
        "        # Update parameters using gradient descent\n",
        "        W_1 -= learning_rate * dW_1\n",
        "        b_1 -= learning_rate * db_1\n",
        "        W_2 -= learning_rate * dW_2\n",
        "        b_2 -= learning_rate * db_2\n",
        "\n",
        "        # Track loss\n",
        "        losses.append(loss)\n",
        "        diff_in_loss = losses[-1] - losses[-2]\n",
        "\n",
        "        # Compute ROC-AUC score\n",
        "        y_hat, _, _, _ = forward_propagation(X=X, W_1=W_1, b_1=b_1, W_2=W_2, b_2=b_2)\n",
        "        roc_auc = roc_auc_score(y_true=y, y_score=y_hat)\n",
        "        roc_auc_scores.append(roc_auc)\n",
        "\n",
        "        # Print progress\n",
        "        if verbose and iteration % 100 == 0:\n",
        "            print(f\"Iteration {iteration:4d}: Loss = {loss:.6f}, ROC-AUC = {roc_auc:.6f}\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nTraining completed after {iteration} iterations\")\n",
        "        print(f\"Final loss: {losses[-1]:.6f}\")\n",
        "        print(f\"Final ROC-AUC: {roc_auc_scores[-1]:.6f}\")\n",
        "\n",
        "    return W_1, b_1, W_2, b_2, losses[1:], roc_auc_scores[1:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the Network\n",
        "\n",
        "Let's train our 2-layer network on the breast cancer data!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the network\n",
        "W_1, b_1, W_2, b_2, losses, roc_auc_scores = train_neural_network(\n",
        "    X=X,\n",
        "    y=y,\n",
        "    W_1_init=W_1_init,\n",
        "    b_1_init=b_1_init,\n",
        "    W_2_init=W_2_init,\n",
        "    b_2_init=b_2_init,\n",
        "    learning_rate=0.1,\n",
        "    epsilon=1e-3,\n",
        "    max_iterations=10000,\n",
        "    verbose=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Training Progress\n",
        "\n",
        "Let's plot how the loss and ROC-AUC score evolved during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss curve\n",
        "axes[0].plot(losses, linewidth=2)\n",
        "axes[0].set_xlabel(\"Iteration\", fontsize=12)\n",
        "axes[0].set_ylabel(\"Loss\", fontsize=12)\n",
        "axes[0].set_title(\"Training Loss Over Time\", fontsize=14)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# ROC-AUC curve\n",
        "axes[1].plot(roc_auc_scores, linewidth=2, color=\"green\")\n",
        "axes[1].set_xlabel(\"Iteration\", fontsize=12)\n",
        "axes[1].set_ylabel(\"ROC-AUC Score\", fontsize=12)\n",
        "axes[1].set_title(\"ROC-AUC Score Over Time\", fontsize=14)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].axhline(y=1.0, color=\"r\", linestyle=\"--\", alpha=0.5, label=\"Perfect score\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Loss decreased from {losses[0]:.4f} to {losses[-1]:.4f}\")\n",
        "print(f\"ROC-AUC improved from {roc_auc_scores[0]:.4f} to {roc_auc_scores[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating on Test Data\n",
        "\n",
        "The real test of our model is how well it performs on unseen data!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "y_test_hat, _, _, _ = forward_propagation(\n",
        "    X=X_test, W_1=W_1, b_1=b_1, W_2=W_2, b_2=b_2\n",
        ")\n",
        "test_roc_auc = roc_auc_score(y_true=y_test, y_score=y_test_hat)\n",
        "\n",
        "print(f\"Test Set Performance:\")\n",
        "print(f\"  ROC-AUC Score: {test_roc_auc:.4f}\")\n",
        "print(f\"\\nThis indicates {'excellent' if test_roc_auc > 0.95 else 'good' if test_roc_auc > 0.85 else 'moderate'} performance!\")\n",
        "\n",
        "# Compare train vs test\n",
        "print(f\"\\nTrain ROC-AUC: {roc_auc_scores[-1]:.4f}\")\n",
        "print(f\"Test ROC-AUC:  {test_roc_auc:.4f}\")\n",
        "print(f\"Difference:    {abs(roc_auc_scores[-1] - test_roc_auc):.4f}\")\n",
        "\n",
        "if abs(roc_auc_scores[-1] - test_roc_auc) < 0.05:\n",
        "    print(\"✓ Good generalization - train and test performance are similar\")\n",
        "else:\n",
        "    print(\"⚠ Possible overfitting - significant gap between train and test performance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the Same Network in PyTorch\n",
        "\n",
        "While implementing neural networks from scratch is educational, modern deep learning frameworks like **PyTorch** make it much easier to build, train, and deploy neural networks.\n",
        "\n",
        "### Why PyTorch?\n",
        "\n",
        "- **Automatic differentiation**: No need to manually derive and code backpropagation\n",
        "- **GPU acceleration**: Easily train on GPUs for faster computation\n",
        "- **Rich ecosystem**: Pre-built layers, optimizers, and tools\n",
        "- **Industry standard**: Widely used in research and production\n",
        "\n",
        "Let's rebuild our 2-layer network using PyTorch!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the Neural Network Architecture\n",
        "\n",
        "In PyTorch, we define a network by subclassing `nn.Module` and specifying the layers in `__init__` and the forward pass in `forward`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwoLayerNN(nn.Module):\n",
        "    \"\"\"A 2-layer neural network for binary classification.\n",
        "\n",
        "    Architecture:\n",
        "        Input -> Hidden Layer (3 neurons, sigmoid) -> Output Layer (1 neuron, sigmoid)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int):\n",
        "        \"\"\"Initializes the network layers.\n",
        "\n",
        "        Args:\n",
        "            input_size: Number of input features.\n",
        "            hidden_size: Number of neurons in the hidden layer.\n",
        "        \"\"\"\n",
        "        super(TwoLayerNN, self).__init__()\n",
        "\n",
        "        # Define layers\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "        self.output = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Performs forward propagation.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, input_size).\n",
        "\n",
        "        Returns:\n",
        "            Output predictions of shape (batch_size, 1).\n",
        "        \"\"\"\n",
        "        # Hidden layer: linear transformation + sigmoid activation\n",
        "        hidden_out = self.sigmoid(self.hidden(x))\n",
        "\n",
        "        # Output layer: linear transformation + sigmoid activation\n",
        "        output = self.sigmoid(self.output(hidden_out))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Create the model\n",
        "input_size = X.shape[1]  # 10 features\n",
        "hidden_size = 3  # 3 hidden neurons\n",
        "\n",
        "model = TwoLayerNN(input_size=input_size, hidden_size=hidden_size).to(device)\n",
        "\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing Data for PyTorch\n",
        "\n",
        "PyTorch works with tensors (similar to NumPy arrays). We'll convert our data and create data loaders for batch processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert NumPy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X).to(device)\n",
        "y_train_tensor = torch.FloatTensor(y).to(device)\n",
        "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
        "\n",
        "# Create datasets and data loaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Batch size: 32\")\n",
        "print(f\"Samples per epoch: {len(train_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the PyTorch Model\n",
        "\n",
        "We need to define:\n",
        "1. **Loss function**: Binary cross-entropy loss\n",
        "2. **Optimizer**: SGD (Stochastic Gradient Descent) or Adam\n",
        "3. **Training loop**: Forward pass → compute loss → backward pass → update weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "pytorch_losses = []\n",
        "pytorch_roc_auc_scores = []\n",
        "\n",
        "print(\"Training PyTorch model...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Iterate over batches\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(batch_X)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(predictions, batch_y)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    pytorch_losses.append(avg_loss)\n",
        "\n",
        "    # Compute ROC-AUC on full training set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_predictions = model(X_train_tensor)\n",
        "        train_roc_auc = roc_auc_score(\n",
        "            y_train_tensor.cpu().numpy(),\n",
        "            train_predictions.cpu().numpy()\n",
        "        )\n",
        "        pytorch_roc_auc_scores.append(train_roc_auc)\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1:3d}/{num_epochs}: Loss = {avg_loss:.6f}, ROC-AUC = {train_roc_auc:.6f}\")\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Final loss: {pytorch_losses[-1]:.6f}\")\n",
        "print(f\"Final ROC-AUC: {pytorch_roc_auc_scores[-1]:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the PyTorch Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_test_pred_pytorch = model(X_test_tensor)\n",
        "    test_roc_auc_pytorch = roc_auc_score(\n",
        "        y_test_tensor.cpu().numpy(),\n",
        "        y_test_pred_pytorch.cpu().numpy()\n",
        "    )\n",
        "\n",
        "print(f\"PyTorch Model - Test Set Performance:\")\n",
        "print(f\"  ROC-AUC Score: {test_roc_auc_pytorch:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing Manual Implementation vs PyTorch\n",
        "\n",
        "Let's visualize and compare the training curves from both approaches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss comparison\n",
        "axes[0].plot(losses[:100], label=\"Manual Implementation\", linewidth=2, alpha=0.8)\n",
        "axes[0].plot(pytorch_losses, label=\"PyTorch\", linewidth=2, alpha=0.8)\n",
        "axes[0].set_xlabel(\"Epoch/Iteration\", fontsize=12)\n",
        "axes[0].set_ylabel(\"Loss\", fontsize=12)\n",
        "axes[0].set_title(\"Training Loss Comparison\", fontsize=14)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# ROC-AUC comparison\n",
        "axes[1].plot(roc_auc_scores[:100], label=\"Manual Implementation\", linewidth=2, alpha=0.8)\n",
        "axes[1].plot(pytorch_roc_auc_scores, label=\"PyTorch\", linewidth=2, alpha=0.8)\n",
        "axes[1].set_xlabel(\"Epoch/Iteration\", fontsize=12)\n",
        "axes[1].set_ylabel(\"ROC-AUC Score\", fontsize=12)\n",
        "axes[1].set_title(\"ROC-AUC Score Comparison\", fontsize=14)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print comparison\n",
        "print(\"\\nFinal Comparison:\")\n",
        "print(f\"{'Method':<25} {'Test ROC-AUC':<15}\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"{'Manual Implementation':<25} {test_roc_auc:.4f}\")\n",
        "print(f\"{'PyTorch':<25} {test_roc_auc_pytorch:.4f}\")\n",
        "print(f\"\\nBoth implementations achieve similar performance!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we've covered:\n",
        "\n",
        "1. **Backpropagation Theory**: Understanding how gradients flow backward through the network using the chain rule\n",
        "2. **Manual Implementation**: Implementing backpropagation and gradient descent from scratch\n",
        "3. **Training Process**: Iteratively updating weights to minimize loss and improve predictions\n",
        "4. **PyTorch Implementation**: Building and training the same network using modern deep learning tools\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Backpropagation** efficiently computes gradients by applying the chain rule backward through the network\n",
        "- **Gradient descent** uses these gradients to iteratively update parameters in the direction that reduces loss\n",
        "- **Binary cross-entropy loss** is the standard loss function for binary classification\n",
        "- **PyTorch** automates the backpropagation process and provides GPU acceleration and rich tools\n",
        "- Both manual and PyTorch implementations achieve similar performance, validating our understanding\n",
        "\n",
        "### Manual vs PyTorch: When to Use Each\n",
        "\n",
        "**Manual Implementation**:\n",
        "- Educational purposes (understanding how things work)\n",
        "- Research on novel architectures or training algorithms\n",
        "- Full control over every detail\n",
        "\n",
        "**PyTorch** (or other frameworks):\n",
        "- Production systems (faster development, better maintenance)\n",
        "- Complex architectures (automatic differentiation is invaluable)\n",
        "- GPU acceleration (essential for large-scale problems)\n",
        "- Industry best practices (optimized implementations)\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "To improve neural network performance, you can explore:\n",
        "- **Deeper networks**: Add more hidden layers\n",
        "- **Different activation functions**: ReLU, tanh, etc.\n",
        "- **Regularization**: Dropout, L2 regularization to prevent overfitting\n",
        "- **Better optimizers**: Adam, RMSprop instead of vanilla SGD\n",
        "- **Advanced architectures**: Convolutional networks (CNNs), Recurrent networks (RNNs), Transformers\n",
        "\n",
        "Congratulations! You now understand the fundamentals of neural networks and how to train them!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
