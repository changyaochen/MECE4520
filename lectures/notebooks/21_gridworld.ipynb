{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4338242",
   "metadata": {},
   "source": [
    "\n",
    "# Gridworld (5×5): Value Iteration → V* and Greedy Policy\n",
    "\n",
    "- Actions: ↑ ↓ ← →\n",
    "- Hitting a wall: stay, reward −1\n",
    "- Teleporters: **A→A'** (+10), **B→B'** (+5)\n",
    "- Discount: γ = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41869082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "H, W = 5, 5\n",
    "gamma = 0.9\n",
    "A, A_prime = (0,1), (4,1)\n",
    "B, B_prime = (0,3), (2,3)\n",
    "A_reward, B_reward = 10.0, 5.0\n",
    "\n",
    "ACTIONS = [(-1,0),(1,0),(0,-1),(0,1)]\n",
    "A_NAMES = ['↑','↓','←','→']\n",
    "\n",
    "def in_bounds(r,c):\n",
    "    return 0 <= r < H and 0 <= c < W\n",
    "\n",
    "def step(state, aidx):\n",
    "    r,c = state\n",
    "    if (r,c) == A:\n",
    "        return A_prime, A_reward\n",
    "    if (r,c) == B:\n",
    "        return B_prime, B_reward\n",
    "    dr,dc = ACTIONS[aidx]\n",
    "    nr, nc = r+dr, c+dc\n",
    "    if in_bounds(nr,nc):\n",
    "        return (nr,nc), 0.0\n",
    "    else:\n",
    "        return (r,c), -1.0\n",
    "\n",
    "def value_iteration(theta=1e-6, max_iter=10000):\n",
    "    V = np.zeros((H,W), dtype=float)\n",
    "    for it in range(max_iter):\n",
    "        delta = 0.0\n",
    "        V_old = V.copy()\n",
    "        for r in range(H):\n",
    "            for c in range(W):\n",
    "                q_vals = []\n",
    "                for a in range(len(ACTIONS)):\n",
    "                    (nr, nc), rwd = step((r,c), a)\n",
    "                    q_vals.append(rwd + gamma * V_old[nr, nc])\n",
    "                V[r,c] = np.max(q_vals)\n",
    "                delta = max(delta, abs(V[r,c]-V_old[r,c]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    Pi = np.zeros((H,W), dtype=int)\n",
    "    for r in range(H):\n",
    "        for c in range(W):\n",
    "            q_vals = []\n",
    "            for a in range(len(ACTIONS)):\n",
    "                (nr, nc), rwd = step((r,c), a)\n",
    "                q_vals.append(rwd + gamma * V[nr, nc])\n",
    "            Pi[r,c] = int(np.argmax(q_vals))\n",
    "    return V, Pi\n",
    "\n",
    "V, Pi = value_iteration()\n",
    "V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.imshow(V, origin=\"upper\")\n",
    "plt.colorbar(label=\"V*\")\n",
    "plt.title(\"Optimal State Values V*\")\n",
    "for r in range(H):\n",
    "    for c in range(W):\n",
    "        plt.text(c, r, f\"{V[r,c]:.1f}\", ha=\"center\", va=\"center\")\n",
    "plt.xticks(range(W))\n",
    "plt.yticks(range(H))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c8902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.imshow(V, origin=\"upper\")\n",
    "plt.title(\"Greedy Policy from V*\")\n",
    "for r in range(H):\n",
    "    for c in range(W):\n",
    "        plt.text(c, r, A_NAMES[Pi[r,c]], ha=\"center\", va=\"center\")\n",
    "plt.xticks(range(W))\n",
    "plt.yticks(range(H))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a1daa",
   "metadata": {},
   "source": [
    "\n",
    "**Exercises**\n",
    "1. Change γ and re-run. How do values/policy shift?\n",
    "2. Change the wall penalty to −2 or −5.\n",
    "3. Remove teleporters A/B; compare the baseline world.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
