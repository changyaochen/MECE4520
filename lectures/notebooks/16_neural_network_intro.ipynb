{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Networks: Introduction and Forward Propagation\n",
        "\n",
        "## What is a Neural Network?\n",
        "\n",
        "A **neural network (NN)** is a computational model inspired by the structure and function of the human brain. \n",
        "At its core, a neural network is a sophisticated function that learns to map inputs to outputs through examples.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "- **Neurons**: The basic computational units that receive inputs, process them, and produce an output\n",
        "- **Layers**: Neurons are organized in layers:\n",
        "  - **Input layer**: Receives the raw data\n",
        "  - **Hidden layer(s)**: Intermediate layers that transform the data\n",
        "  - **Output layer**: Produces the final prediction\n",
        "- **Weights and Biases**: Learnable parameters that control the transformation at each layer\n",
        "- **Activation Functions**: Non-linear functions applied to neuron outputs (e.g., sigmoid, ReLU)\n",
        "\n",
        "### Why Neural Networks?\n",
        "\n",
        "Neural networks excel at:\n",
        "- Learning complex, non-linear patterns in data\n",
        "- Automatic feature extraction (no manual feature engineering needed)\n",
        "- Handling high-dimensional data (images, text, audio)\n",
        "- Achieving state-of-the-art performance on many tasks\n",
        "\n",
        "In this notebook, we'll build a **2-layer neural network** for binary classification using the breast cancer dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "We'll use the **breast cancer dataset** for binary classification. \n",
        "The task is to predict whether a tumor is **benign (B)** or **malignant (M)** based on various features extracted from cell nuclei images.\n",
        "\n",
        "This is a real-world medical diagnosis problem where:\n",
        "- **Input**: 30 numerical features describing cell characteristics\n",
        "- **Output**: Binary label (0 = benign, 1 = malignant)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the breast cancer dataset\n",
        "data = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/changyaochen/MECE4520/master/\"\n",
        "    \"data/breast_cancer.csv\"\n",
        ")\n",
        "\n",
        "# Convert diagnosis to binary label: 0 = Benign, 1 = Malignant\n",
        "data[\"label\"] = data[\"diagnosis\"].apply(lambda x: 0 if x == \"B\" else 1)\n",
        "\n",
        "# Display first few rows\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(data[\"label\"].value_counts())\n",
        "print(f\"\\nSample data:\")\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Selection and Preprocessing\n",
        "\n",
        "We'll select 10 key features that describe the tumor characteristics. Before feeding data into a neural network, it's crucial to:\n",
        "\n",
        "1. **Standardize the features**: Neural networks are sensitive to the scale of input features. Standardization (zero mean, unit variance) ensures:\n",
        "   - Faster convergence during training\n",
        "   - More stable gradient computations\n",
        "   - Equal importance given to all features initially\n",
        "\n",
        "2. **Split into train/test sets**: We need separate data for training and evaluation to assess generalization performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for the model\n",
        "features = [\n",
        "    \"radius_mean\",\n",
        "    \"texture_mean\",\n",
        "    \"perimeter_mean\",\n",
        "    \"area_mean\",\n",
        "    \"smoothness_mean\",\n",
        "    \"compactness_mean\",\n",
        "    \"concavity_mean\",\n",
        "    \"concave_mean\",\n",
        "    \"symmetry_mean\",\n",
        "    \"fractal_mean\",\n",
        "]\n",
        "label = \"label\"\n",
        "\n",
        "# Split into train and test sets (80/20 split)\n",
        "X_raw, X_raw_test, y, y_test = train_test_split(\n",
        "    data[features].values,\n",
        "    data[label].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {X_raw.shape[0]}\")\n",
        "print(f\"Test samples: {X_raw_test.shape[0]}\")\n",
        "print(f\"Number of features: {X_raw.shape[1]}\")\n",
        "\n",
        "# Standardize the features\n",
        "# Important: Fit the scaler only on training data to avoid data leakage\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_raw)\n",
        "X = scaler.transform(X_raw)\n",
        "X_test = scaler.transform(X_raw_test)\n",
        "\n",
        "# Reshape labels to column vectors for matrix operations\n",
        "y = y.reshape((-1, 1))\n",
        "y_test = y_test.reshape((-1, 1))\n",
        "\n",
        "print(f\"\\nAfter preprocessing:\")\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Architecture\n",
        "\n",
        "We'll construct a **2-layer neural network** with the following structure:\n",
        "\n",
        "```\n",
        "Input Layer (10 features)\n",
        "       ↓\n",
        "Hidden Layer (3 neurons with sigmoid activation)\n",
        "       ↓\n",
        "Output Layer (1 neuron with sigmoid activation)\n",
        "```\n",
        "\n",
        "### Understanding the Architecture\n",
        "\n",
        "- **Layer 1 (Hidden Layer)**:\n",
        "  - Takes 10 input features\n",
        "  - Has 3 neurons (hidden units)\n",
        "  - Each neuron computes: \\( a = \\sigma(w^T x + b) \\)\n",
        "  - Parameters: \\( W_1 \\) (3×10 matrix) and \\( b_1 \\) (3×1 vector)\n",
        "\n",
        "- **Layer 2 (Output Layer)**:\n",
        "  - Takes 3 inputs from the hidden layer\n",
        "  - Has 1 neuron (for binary classification)\n",
        "  - Computes: \\( \\hat{y} = \\sigma(w^T a + b) \\)\n",
        "  - Parameters: \\( W_2 \\) (1×3 matrix) and \\( b_2 \\) (scalar)\n",
        "\n",
        "### Why These Shapes?\n",
        "\n",
        "The matrix dimensions are designed so that matrix multiplication works correctly:\n",
        "- \\( W_1 \\): (3, 10) × (10, n_samples) = (3, n_samples)\n",
        "- \\( W_2 \\): (1, 3) × (3, n_samples) = (1, n_samples)\n",
        "\n",
        "This allows us to process all samples in a batch efficiently!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Sigmoid Activation Function\n",
        "\n",
        "The **sigmoid function** \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\) is used to introduce non-linearity into the network.\n",
        "\n",
        "**Properties:**\n",
        "- Maps any real number to the range (0, 1)\n",
        "- Smooth and differentiable (important for training)\n",
        "- Interpretable as a probability for binary classification\n",
        "\n",
        "Without activation functions, stacking multiple layers would just be equivalent to a single linear transformation!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Computes the sigmoid activation function.\n",
        "\n",
        "    The sigmoid function maps any real-valued number to the range (0, 1),\n",
        "    making it suitable for binary classification problems.\n",
        "\n",
        "    Args:\n",
        "        z: Input array of any shape.\n",
        "\n",
        "    Returns:\n",
        "        Array of same shape as z with sigmoid applied element-wise.\n",
        "    \"\"\"\n",
        "    return 1.0 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "# Visualize the sigmoid function\n",
        "z_values = np.linspace(-10, 10, 200)\n",
        "sigmoid_values = sigmoid(z_values)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(z_values, sigmoid_values, linewidth=2)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlabel(\"Input (z)\", fontsize=12)\n",
        "plt.ylabel(\"Sigmoid(z)\", fontsize=12)\n",
        "plt.title(\"Sigmoid Activation Function\", fontsize=14)\n",
        "plt.axhline(y=0.5, color=\"r\", linestyle=\"--\", alpha=0.5, label=\"Decision boundary\")\n",
        "plt.axvline(x=0, color=\"r\", linestyle=\"--\", alpha=0.5)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parameter Initialization\n",
        "\n",
        "We initialize the weights and biases with small random values:\n",
        "- **Weights**: Random values from a normal distribution\n",
        "- **Biases**: Random values (or zeros) \n",
        "\n",
        "Random initialization is crucial to break symmetry - if all weights were the same, all neurons in a layer would learn the same features!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize parameters for Layer 1 (Hidden Layer)\n",
        "# W_1: (3, 10) - connects 10 inputs to 3 hidden neurons\n",
        "# b_1: (3, 1) - bias for each of 3 hidden neurons\n",
        "np.random.seed(10)  # For reproducibility\n",
        "\n",
        "W_1 = np.random.normal(size=(3, X.shape[1]))\n",
        "b_1 = np.random.normal(size=(3, 1))\n",
        "\n",
        "print(\"Layer 1 (Hidden Layer) Parameters:\")\n",
        "print(f\"  W_1 shape: {W_1.shape} - Weight matrix\")\n",
        "print(f\"  b_1 shape: {b_1.shape} - Bias vector\")\n",
        "\n",
        "# Initialize parameters for Layer 2 (Output Layer)\n",
        "# W_2: (1, 3) - connects 3 hidden neurons to 1 output\n",
        "# b_2: (1, 1) - bias for the output neuron\n",
        "W_2 = np.random.normal(size=(1, 3))\n",
        "b_2 = np.random.normal(size=(1, 1))\n",
        "\n",
        "print(\"\\nLayer 2 (Output Layer) Parameters:\")\n",
        "print(f\"  W_2 shape: {W_2.shape} - Weight matrix\")\n",
        "print(f\"  b_2 shape: {b_2.shape} - Bias scalar\")\n",
        "\n",
        "# Total number of parameters\n",
        "total_params = W_1.size + b_1.size + W_2.size + b_2.size\n",
        "print(f\"\\nTotal trainable parameters: {total_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forward Propagation\n",
        "\n",
        "**Forward propagation** is the process of computing the output of a neural network given an input. Information flows forward through the network from input to output.\n",
        "\n",
        "### The Process (for our 2-layer network):\n",
        "\n",
        "1. **Layer 1 Computation**:\n",
        "   - Compute linear combination: \\( Z_1 = X W_1^T + b_1^T \\)\n",
        "   - Apply activation: \\( A_1 = \\sigma(Z_1) \\)\n",
        "\n",
        "2. **Layer 2 Computation**:\n",
        "   - Compute linear combination: \\( Z_2 = A_1 W_2^T + b_2^T \\)\n",
        "   - Apply activation: \\( A_2 = \\hat{Y} = \\sigma(Z_2) \\)\n",
        "\n",
        "The final output \\( \\hat{Y} \\) contains predicted probabilities for each sample.\n",
        "\n",
        "### Matrix Dimensions Flow\n",
        "\n",
        "For a batch of 455 training samples:\n",
        "- Input \\( X \\): (455, 10)\n",
        "- After Layer 1: \\( Z_1 \\): (455, 3), \\( A_1 \\): (455, 3)\n",
        "- After Layer 2: \\( Z_2 \\): (455, 1), \\( A_2 \\): (455, 1)\n",
        "\n",
        "Let's implement this step by step!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Forward propagation - Layer 1 (Hidden Layer)\n",
        "print(\"=\" * 60)\n",
        "print(\"LAYER 1: HIDDEN LAYER COMPUTATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Step 1a: Linear transformation\n",
        "# Z_1 = X @ W_1.T + b_1.T\n",
        "# Each row of X (a sample) is multiplied by W_1.T to get 3 values\n",
        "Z_1 = X @ W_1.T\n",
        "print(f\"\\nStep 1a - Linear transformation (Z_1 = X @ W_1.T):\")\n",
        "print(f\"  Input shape: {X.shape}\")\n",
        "print(f\"  Weight matrix shape: {W_1.T.shape}\")\n",
        "print(f\"  Output Z_1 shape: {Z_1.shape}\")\n",
        "print(f\"\\n  Sample Z_1 values (first 3 samples):\")\n",
        "print(Z_1[:3])\n",
        "\n",
        "# Step 1b: Add bias and apply activation\n",
        "# A_1 = sigmoid(Z_1 + b_1.T)\n",
        "A_1 = sigmoid(Z_1 + b_1.T)\n",
        "print(f\"\\nStep 1b - Add bias and apply sigmoid (A_1 = sigmoid(Z_1 + b_1.T)):\")\n",
        "print(f\"  Bias shape: {b_1.T.shape}\")\n",
        "print(f\"  Output A_1 shape: {A_1.shape}\")\n",
        "print(f\"\\n  Sample A_1 values (first 3 samples):\")\n",
        "print(A_1[:3])\n",
        "print(f\"\\n  A_1 contains {A_1.shape[1]} activations per sample\")\n",
        "print(f\"  (one for each hidden neuron)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Forward propagation - Layer 2 (Output Layer)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LAYER 2: OUTPUT LAYER COMPUTATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Step 2a: Linear transformation\n",
        "# Z_2 = A_1 @ W_2.T\n",
        "Z_2 = A_1 @ W_2.T\n",
        "print(f\"\\nStep 2a - Linear transformation (Z_2 = A_1 @ W_2.T):\")\n",
        "print(f\"  Input shape: {A_1.shape}\")\n",
        "print(f\"  Weight matrix shape: {W_2.T.shape}\")\n",
        "print(f\"  Output Z_2 shape: {Z_2.shape}\")\n",
        "print(f\"\\n  Sample Z_2 values (first 3 samples):\")\n",
        "print(Z_2[:3])\n",
        "\n",
        "# Step 2b: Add bias and apply activation\n",
        "# A_2 = Y_hat = sigmoid(Z_2 + b_2.T)\n",
        "A_2 = y_hat = sigmoid(Z_2 + b_2.T)\n",
        "print(f\"\\nStep 2b - Add bias and apply sigmoid (A_2 = sigmoid(Z_2 + b_2.T)):\")\n",
        "print(f\"  Bias shape: {b_2.T.shape}\")\n",
        "print(f\"  Output A_2 (Y_hat) shape: {A_2.shape}\")\n",
        "print(f\"\\n  Sample predictions (first 5 samples):\")\n",
        "print(A_2[:5])\n",
        "print(f\"\\n  These are predicted probabilities for class 1 (malignant)\")\n",
        "print(f\"  Values close to 1 = likely malignant\")\n",
        "print(f\"  Values close to 0 = likely benign\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a Forward Propagation Function\n",
        "\n",
        "Now let's package the forward propagation logic into a reusable function. This will be useful when we train the network in the next notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward_propagation(\n",
        "    X: np.ndarray,\n",
        "    W_1: np.ndarray,\n",
        "    b_1: np.ndarray,\n",
        "    W_2: np.ndarray,\n",
        "    b_2: np.ndarray,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Performs forward propagation through a 2-layer neural network.\n",
        "\n",
        "    Computes the activations for each layer by applying linear transformations\n",
        "    followed by sigmoid activations. Returns all intermediate values which are\n",
        "    needed for backpropagation during training.\n",
        "\n",
        "    Args:\n",
        "        X: Input data of shape (n_samples, n_features).\n",
        "        W_1: Weight matrix for layer 1 of shape (n_hidden, n_features).\n",
        "        b_1: Bias vector for layer 1 of shape (n_hidden, 1).\n",
        "        W_2: Weight matrix for layer 2 of shape (1, n_hidden).\n",
        "        b_2: Bias scalar for layer 2 of shape (1, 1).\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "            - A_2: Final output predictions of shape (n_samples, 1).\n",
        "            - Z_2: Pre-activation values for layer 2 of shape (n_samples, 1).\n",
        "            - A_1: Activations from layer 1 of shape (n_samples, n_hidden).\n",
        "            - Z_1: Pre-activation values for layer 1 of shape (n_samples, n_hidden).\n",
        "    \"\"\"\n",
        "    # Layer 1: Hidden layer computation\n",
        "    Z_1 = X @ W_1.T\n",
        "    A_1 = sigmoid(Z_1 + b_1.T)\n",
        "\n",
        "    # Layer 2: Output layer computation\n",
        "    Z_2 = A_1 @ W_2.T\n",
        "    A_2 = sigmoid(Z_2 + b_2.T)\n",
        "\n",
        "    return A_2, Z_2, A_1, Z_1\n",
        "\n",
        "\n",
        "# Test the function\n",
        "y_hat, Z_2, A_1, Z_1 = forward_propagation(X=X, W_1=W_1, b_1=b_1, W_2=W_2, b_2=b_2)\n",
        "\n",
        "print(\"Forward propagation function output:\")\n",
        "print(f\"  y_hat shape: {y_hat.shape}\")\n",
        "print(f\"  First 5 predictions: {y_hat[:5].flatten()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we've covered:\n",
        "\n",
        "1. **Neural Network Fundamentals**: Understanding the architecture, components, and why neural networks are powerful\n",
        "2. **Data Preparation**: Loading and preprocessing the breast cancer dataset with proper standardization\n",
        "3. **Network Architecture**: Designing a 2-layer network with 3 hidden neurons\n",
        "4. **Forward Propagation**: Implementing the forward pass to compute predictions from inputs\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- Neural networks transform inputs through layers of weighted connections and non-linear activations\n",
        "- Matrix operations allow efficient computation for batches of samples\n",
        "- The sigmoid activation function introduces non-linearity and bounds outputs to (0, 1)\n",
        "- Forward propagation computes predictions, but with random weights, predictions are not yet accurate\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "The predictions from our randomly initialized network are not useful yet! In the next notebook (**17_neural_network_training.ipynb**), we'll learn:\n",
        "- How to measure prediction quality using a loss function\n",
        "- **Backpropagation**: Computing gradients to update parameters\n",
        "- **Training**: Using gradient descent to learn optimal weights\n",
        "- Implementing the same network in **PyTorch** for easier development\n",
        "\n",
        "With proper training, our network will learn to accurately classify tumors!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
