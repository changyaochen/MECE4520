{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e05a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\", {'grid.linestyle':'--'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0544c2a3",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "Linear regression is a statistical method and a fundamental type of predictive analytical model used to describe the relationship between a dependent variable and one or more independent variables. In this class, we will use the \"Auto MPG\" dataset to learn and practice linear regression.\n",
    "\n",
    "## The data\n",
    "\n",
    "The \"Auto MPG\" dataset is one of the classic datasets available in the UCI Machine Learning Repository. It contains city-cycle fuel consumption estimates for various automobiles produced in the 1970s and 1980s, making it a commonly used dataset for regression analysis tasks in machine learning.\n",
    "\n",
    "### Data Set Information:\n",
    "\n",
    "The data concerns city-cycle fuel consumption in miles per gallon (MPG) to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.\n",
    "\n",
    "* mpg: continuous\n",
    "* cylinders: multi-valued discrete\n",
    "* displacement: continuous\n",
    "* horsepower: continuous\n",
    "* weight: continuous\n",
    "* acceleration: continuous\n",
    "* model year: multi-valued discrete\n",
    "* origin: multi-valued discrete\n",
    "* car name: string (unique for each instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a66c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/changyaochen/MECE4520/master/data/auto_mpg.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42c6df",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Exploratory Data Analysis (EDA) refers to the process of visually and analytically inspecting data to uncover its main characteristics, often using statistical graphics, plots, and information tables. EDA is essential in data science, and is a fundamental step before proceeding to more advanced machine learning or statistical modeling, as it helps analysts and researchers understand the nature of the data, its structure, anomalies, patterns, and relationships.\n",
    "\n",
    "EDA is often an iterative process: as you discover one feature or pattern in your data, it might lead you to look for other related features or patterns. Below, we will use the \"Auto MPG\" dataset as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the dependent variable\n",
    "sns.histplot(x=\"mpg\", data=data)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations\n",
    "continuous_variables = [\n",
    "    \"displacement\",\n",
    "    \"horsepower\",\n",
    "    \"weight\",\n",
    "    \"acceleration\",\n",
    "]\n",
    "\n",
    "for variable in continuous_variables:\n",
    "    plt.figure()\n",
    "    sns.scatterplot(x=variable, y=\"mpg\", data=data)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e2100",
   "metadata": {},
   "source": [
    "## Simple linear regression\n",
    "\n",
    "Simple linear regression refers to the special case of linear regression where there is only a single independent variable. Namely, it takes the form of:\n",
    "$$y = \\beta_0 + \\beta_1 x,$$\n",
    "where $y$ is the dependent variable, and $x$ is the single independent variable. The goal of the linear regression is to find the best values for the intercept $\\beta_0$, and the slope $\\beta_1$.\n",
    "\n",
    "Below we will use the \"Auto MPG\" dataset as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4dac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"mpg\"]\n",
    "x = data[\"weight\"]\n",
    "\n",
    "def simple_linear_regression(\n",
    "    x: Union[List, np.ndarray, pd.Series], \n",
    "    y: Union[List, np.ndarray, pd.Series]) -> Tuple[float, float]:\n",
    "    \"\"\"Return the intercept and slope of a simple linear regression.\"\"\"\n",
    "    beta_1 = np.cov(x, y)[0][1] / np.cov(x, x)[0][1]\n",
    "    beta_0 = np.mean(y) - beta_1 * np.mean(x)\n",
    "    \n",
    "    return beta_0, beta_1\n",
    "\n",
    "beta_0, beta_1 = simple_linear_regression(x=x, y=y)\n",
    "\n",
    "# calculate R^2\n",
    "y_pred = beta_0 + beta_1 * x\n",
    "SST = np.sum(np.square(y - np.mean(y)))\n",
    "residual = y - y_pred\n",
    "SSE = np.sum(np.square(residual))\n",
    "r2 = 1 - SSE / SST\n",
    "\n",
    "print(f\"beta_0 is: {beta_0:5.4f}\")\n",
    "print(f\"beta_1 is: {beta_1:5.4f}\")\n",
    "print(f\"R-square is: {r2:5.4f}\")\n",
    "\n",
    "plt.figure()\n",
    "x_range = np.linspace(start=np.min(x), stop=np.max(x), num=100)\n",
    "sns.scatterplot(x=\"weight\", y=\"mpg\", data=data)\n",
    "sns.lineplot(x=x_range, y=(beta_0 + beta_1 * x_range), color=\"red\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f667860-760f-4a60-9766-5b2b26a950d4",
   "metadata": {},
   "source": [
    "Equally important to, if not more important than, the model fitting, is the model/error analysis. Here we will demonstrate a simple residual analysis, to examine if its distirubtion agrees with our (normality) assumption> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180bcbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual analysis\n",
    "plt.figure()\n",
    "sns.histplot(residual)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511fb1c-9427-463e-870d-cd5d009baaee",
   "metadata": {},
   "source": [
    "An important aspect in linear regression is that, the fitted parameters (_e.g._, intercept, slope), are just point estimates of the population parameter. Here the \"population\" refers to the underlying stochastic process that generates the observed data. \n",
    "\n",
    "Below, we will run a simulation: in each trial, we only sample 20% from the full observation, and run the simple linear regression. We will repeat the trial 100 times. This is to show that, the fitting parameters is a random variable: it depends on the exact data that is used. Therefore, as a random variable, we need to pay attention to its bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of the coefficients\n",
    "np.random.seed(42)\n",
    "n_trials = 100\n",
    "\n",
    "beta_0s, beta_1s = [], []\n",
    "for _ in tqdm(range(n_trials)):\n",
    "    sampling_proba = 0.2\n",
    "    mask = np.random.choice([True, False], size=len(x), p=[sampling_proba, 1 - sampling_proba])\n",
    "    x_sampled, y_sampled = x[mask], y[mask]\n",
    "    beta_0, beta_1 = simple_linear_regression(x=x_sampled, y=y_sampled)\n",
    "    beta_0s.append(beta_0)\n",
    "    beta_1s.append(beta_1)\n",
    "\n",
    "# plot the histograms\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 5))\n",
    "sns.histplot(beta_0s, ax=axes[0])\n",
    "sns.histplot(beta_1s, ax=axes[1])\n",
    "plt.tight_layout()\n",
    "\n",
    "# plot the fited lines\n",
    "plt.figure()\n",
    "x_range = np.linspace(start=np.min(x), stop=np.max(x), num=100)\n",
    "for i in tqdm(range(len(beta_0s))):\n",
    "    sns.lineplot(x=x_range, y=(beta_0s[i] + beta_1s[i] * x_range), color=\"red\", alpha=0.1)\n",
    "sns.scatterplot(x=\"weight\", y=\"mpg\", data=data)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence intervals\n",
    "SE_beta_0 = (np.var(residual, ddof=2) * (1. / len(x) + (np.mean(x))**2 / np.sum((x - np.mean(x))**2)))**0.5\n",
    "SE_beta_1 = (np.var(residual) / np.sum((x - np.mean(x))**2))**0.5 \n",
    "\n",
    "print(f\"The standard error for beta_0 is: {SE_beta_0:5.4f}\")\n",
    "print(f\"The standard error for beta_1 is: {SE_beta_1:5.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c59331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple linear regression with the `statsmodels` library\n",
    "model_1 = smf.ols(formula='mpg ~ weight', data=data)\n",
    "result_1 = model_1.fit()\n",
    "print(result_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa93c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regress in matrix format\n",
    "X = np.hstack(\n",
    "    (np.ones(shape=(len(x), 1)), \n",
    "     x.to_numpy().reshape(-1, 1)))\n",
    "\n",
    "# point estimate\n",
    "beta_matrix = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(\"The estimates for beta are:\")\n",
    "print(beta_matrix)\n",
    "\n",
    "# variance\n",
    "se_matrix = np.sqrt(np.var(residual, ddof=2) * np.linalg.inv(X.T @ X))\n",
    "print(\"\\nThe standard error for beta are:\")\n",
    "print(se_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b89fb5",
   "metadata": {},
   "source": [
    "## Multi-variant linear regression\n",
    "\n",
    "Multi-variant linear regression is a linear regression that takes more than one independent variable. \n",
    "\n",
    "In the following, we will use the Python `statsmodels` library to perform multi-variant linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = smf.ols(formula='mpg ~ weight + displacement + horsepower + acceleration', data=data)\n",
    "result_2 = model_2.fit()\n",
    "print(result_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff36c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between the continuous variables\n",
    "data[continuous_variables].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf82d5-a74a-4106-afc5-0fa76a9f8482",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "If the independent variable is of categorical type, we can not directly use them in regression. One-hot encoding is the solution to address this issue. \n",
    "\n",
    "At its core, one-hot encoding is about converting nominal categorical data into a format that can be used by machine learning algorithms. Categorical data refers to variables that can be divided into multiple categories but have no order or priority. These categories can be text labels, which many ML algorithms can't work with directly because they expect numerical input features.\n",
    "\n",
    "With the `statsmodels` library, you simply wrap the independent variable with `C()`. If you are using other libraries, please refer to the corresponding APIs. For example, in the popular `scikit-learn` library, one can [use](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) the `OneHotEncoder` to achieve the same goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the categorical variables\n",
    "model_3 = smf.ols(formula='mpg ~ weight + C(origin)', data=data)\n",
    "result_3 = model_3.fit()\n",
    "print(result_3.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
